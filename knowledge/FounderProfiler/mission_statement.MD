# Adaptive Entrepreneur Profiler – Engineering Prompt (for Cursor + GPT‑5)

> **You are the coding agent.** Implement an adaptive entrepreneurial profiling flow using the provided JSON assets. Do **not** improvise scoring or selection logic in the model—those rules live in the app logic. Use Supabase for persistence and sessioning.

---

## 1) Goal

Deliver an **adaptive interview** that classifies a user into one of \~20 sub‑profiles (within 7 parent types) with ≥90% confidence in \~10–14 questions (max 18). Output profile, confidence, top alternatives, and dimension estimates.

---

## 2) Assets & Where to Find Them

* **Folder**: `knowledge/FounderProfile/`
* **Files**:

  * `profiles_config.json`
  * `questions_bank.json`
  * `questions_bank_v1_1.json` *(extended bank for repeat testing)*
  * `README_ADAPTIVE_PROFILER.md`
  * `README_ADAPTIVE_PROFILER_V1_1_ADDENDUM.md`

**Assumption**: These files are bundled with the frontend app at build time (client-readable) or served from Supabase storage (read‑only). Choose the safer approach based on where prompts are exposed.

---

## 3) Environment Variables

* Stored in `.env` (not readable by LLM).
* At minimum:

  * `OPENAI_API_KEY` — already configured in your existing conversational agent.
  * `SUPABASE_URL` and `SUPABASE_ANON_KEY` — for public client operations.
  * `SUPABASE_SERVICE_ROLE_KEY` — for server-side admin tasks (never expose to client).
  * Optional: `PROFILER_STOP_THRESHOLD` (e.g., `0.90` or `0.95`).
  * Optional: `PROFILER_MAX_QUESTIONS` (e.g., `18`).

---

## 4) High‑Level Architecture

**Frontend (App + Orchestrator)**

* Loads JSON assets from `knowledge/FounderProfile/`.
* Maintains **belief state per dimension** (mu, sigma²) locally.
* Chooses the **next question** adaptively (highest uncertainty / info gain heuristic).
* Applies **stop rule** and computes **posterior over profiles**.
* Manages UX: display question, collect free‑text answer, ask for clarifications if needed, show progress and final report.

**LLM (GPT‑5 via API)**

* Role: **conversational layer only**.
* Tasks:

  * Rephrase and ask the current question naturally.
  * Map free‑text replies to the discrete scale **−2, −1, 0, 1, 2** with a JSON‑only response contract.
  * Ask a **single clarifying question** when confidence is low.
  * Produce a concise **final narrative summary** of the result (no math revealed).

**Backend (Supabase)**

* Auth/session.
* Minimal persistence of interview state (optional, if you want resume capability).
* Storage of **results** and **telemetry** (question ids, mapped values, confidences, timestamps).
* Optional: serve the JSON assets from Supabase Storage (read‑only public or RLS‑protected signed URLs).

---

## 5) Responsibilities Split (Do Not Drift)

* **Deterministic logic (App)**: priors, Gaussian updates, next‑question selection, posterior computation, stop rule, confidence thresholds.
* **LLM**: natural phrasing, mapping of free text → one of {−2, −1, 0, 1, 2}, optional clarification, final summary.
* **Supabase**: persistence, auth, storage.

---

## 6) Core Flow (Step‑By‑Step)

1. **Init**: Load the appropriate question bank (default `questions_bank.json` or extended `questions_bank_v1_1.json` if repeat user) plus `profiles_config.json`. Initialize priors per dimension (mu=0, sigma²=1.0; optionally 1.5 for higher initial uncertainty).
2. **Ask**: Select next question based on highest sigma² (tie‑break by which dimension best separates current top‑2 profiles). Render question and collect user reply.
3. **Map** (LLM): Send the question prompt + options to GPT‑5 to obtain `{answer_value, confidence, notes}` or `{need_clarification, clarifying_question}`.
4. **Update**: Apply Bayesian update on the targeted dimension using `answer_value` and question `weight`.
5. **Posterior**: Convert current dimension means into profile probabilities using softmax over negative squared distance to centroids.
6. **Stop**: If top profile probability ≥0.90 (or ≥0.95 strict) **OR** 18 questions asked, stop.
7. **Output**: Display final profile, confidence, top 3 alternatives, dimension estimates, and a short narrative generated by the LLM. Store results in Supabase.

---

## 7) Contracts (LLM I/O)

**System persona** (LLM): Professional entrepreneurial profiler; ask one question at a time; neutral; concise. Always output strict JSON when mapping answers.

**Mapping response (expected JSON):**

* Success: `{ "answer_value": -2|−1|0|1|2, "confidence": 0..1, "notes": "…" }`
* Low confidence: `{ "need_clarification": true, "clarifying_question": "…" }`

**Final summary (expected JSON):**

* `{ "profile": "…", "confidence": 0..1, "alternatives": [{"profile":"…","prob":…}], "dimension_estimates": {"vision_execution": …}, "n_questions": N, "n_clarifications": M, "summary": "…" }`

**Important**: The LLM must not perform scoring, choose questions, or override stop rules.

---

## 8) Supabase Integration (Non‑code Requirements)

* **Auth**: use existing conversational agent auth (already linked with `OPENAI_API_KEY`).
* **Tables (suggested)**:

  * `profiler_sessions`: `id`, `user_id`, `status`, `created_at`, `updated_at`, `n_questions`, `confidence`, `top_profile`, `alternatives_json`, `dimensions_json`.
  * `profiler_events`: `session_id`, `q_id`, `dimension`, `mapped_value`, `llm_confidence`, `timestamp`.
* **Policies**: RLS so users can only read/write their sessions; anonymized aggregate reads for analytics allowed only by service role.
* **Storage**: place JSON assets in Supabase Storage or serve from `knowledge/FounderProfile/` in app build.

---

## 9) UX Requirements

* Show progress (e.g., “Question 5 of \~12”).
* Allow **Skip**; treat skip as `0` (neutral) and mark lower weight.
* Clarifications should be **single‑turn**; if still unclear, fallback to neutral (0) and continue.
* End screen: profile card with strengths, typical pitfalls, and **suggested complementary profiles**.

---

## 10) Safety, Privacy, and Data Handling

* Do not store raw free‑text answers—store only mapped values, question ids, and anonymized telemetry.
* Provide a clear **consent** line for users before starting.
* Respect deletion requests: cascading delete `profiler_events` when a session is deleted.

---

## 11) Configuration & Versioning

* Keep a `profiler_manifest.json` with `version`, `assets_path`, and a checksum for each JSON file.
* On load, verify checksum; if mismatch, abort and log.
* Include a **feature flag** for strict vs. relaxed stop thresholds.

---

## 12) Testing & Acceptance Criteria

* **Determinism**: With a fixed sequence of mapped values, results must be identical.
* **Convergence**: Simulate 1,000 random users and verify median question count ≤14 for ≥90% confidence.
* **Edge cases**: All neutrals; rapid extremes; contradictory answers → still stop ≤18.
* **Resilience**: LLM returns malformed JSON → retry; fallback to deterministic multiple‑choice UI if repeated.

---

## 13) Telemetry (Non‑PII)

* Track: `q_id`, `dimension`, `mapped_value`, `llm_confidence`, `time_to_answer`, `n_clarifications`, `stop_reason`.
* Use aggregates to improve question wording later.

---

## 14) Deliverables Checklist

* Load assets from `knowledge/FounderProfile/`.
* Implement belief state, selection heuristic, stop rule, posterior computation.
* Implement LLM wrappers that only handle phrasing/mapping/summary.
* Wire to Supabase tables with RLS.
* Implement resumeable sessions (optional) and minimal admin telemetry view (optional).
* Ship with an environment toggle for stop threshold.

---

## 15) Out‑of‑Scope

* No personality diagnostics beyond provided dimensions.
* No psychological claims or clinical language.
* No auto‑matching of cofounders in this iteration.

---

## 16) Guardrails for the LLM

* Never create or modify scoring rules.
* Never change the options’ numeric mapping.
* Never reveal internal logits or priors; speak plainly.
* If unsure, ask one clarifying question; otherwise map to closest value.

---

**End of prompt.** Follow strictly. No novel logic without explicit instruction.
